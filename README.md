# Machine Learning Video Gesture Overlay for Emoji Reacts on Zoom
# About
- Created by Ashwat Chidambaram, Andre He, Aryia Dattamajumdar, and Sarvasv Kulpati
- New Member Education Program (NMEP) Final Project, as part of Machine Learning at Berkeley (ML@B)
# Motivation
- As students at UC Berkeley all our classes were held on Zoom amid the COVID-19 pandemic. We often found it quite unnecessary for one to unmute themselves over video calls to simply say a quick word or two. Additionally,privacy is becoming an increasingly prevalent issue in this day and age, so when a user steps away from the camera, our software automatically covers up the entire video feed to prevent oneâ€™s background from being exposed, and subsequently unhides when it detects the user come back into view. Inspired by our frustrations and observations with video-based learning, we created this application to convey a quick messages using emoji reacts without having to interact with computer hardware at all. 

# Building & Running
- 
# Visual Model
- 
# Credits
Hand Tracking Module: Victor Dibia, HandTrack: A Library For Prototyping Real-time Hand Tracking Interfaces using Convolutional Neural Networks, https://github.com/victordibia/handtracking
